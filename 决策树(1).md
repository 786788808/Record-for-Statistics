### 一. 定义：
决策树是一种非参数模型(不想回归，有个基本形式，这更像一个黑盒)，它无须对目标函数和变量做过多的假设，使用更加灵活，能够处理更加复杂场景下的问题。它是一种类似于流程图的树形结构，树内部的每一个节点代表对一个特征的测试，树的分支代表该特征的每一个测试结果，树的每一个叶子节点代表一个类别(决策结果)。    
一般，一棵决策树包含一个根节点、若干个内部节点和若干个叶节点。  
决策树学习的目的是为了产生一棵泛化能力强，即处理未见示例能力强的决策树。  

### 二. 划分选择：
前提知识：信息熵：事件的不确定性，是度量样本集合纯度最常用的一种指标。可以理解为：当某件事很复杂，有多种决策情况，熵就大；当事件很简单，决策单一，熵就小。
ID3和C4.5对于特征的选取都是基于信息熵的，在不断生成新枝的时候，熵也不断降低。   

ID3:  
用**信息增益**来选择合适的特征(划分属性)作为节点。  
当信息增益越大，就意味着使用该属性来进行划分所获得的纯度提升越大，thus，优先选这个属性。    
例子见西瓜书，很容易理解。  
缺点：信息增益会偏向选择取值数目较多的属性。  

C4.5:  
用**信息增益和信息增益率**来选择合适的特征作为节点。单单用信息增益率的话，会偏向于选择数目较少的特征。所以，算法先从信息增益率高于平均水平的属性，再从中选择增益率最高的特征。   

CART:  
使用基尼指数(Gini index)来选择划分属性。  
基尼指数越小，数据集的纯度越高。选择使得划分后基尼指数最小的属性作为最优划分属性。  

说明：  
ID3、C4.5只能处理分类问题，CART可处理分类和回归问题。
ID3不可处理连续值，C4.5和CART可以处理连续值，但是C4.5实现会比较复杂。   
在实际工作中，CART的应用更广泛，sklearn中默认用这个。  

### 三. 算法优缺点：



