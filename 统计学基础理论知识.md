![](https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fc-ssl.duitang.com%2Fuploads%2Fitem%2F201908%2F09%2F20190809222848_dvxth.thumb.400_0.jpg&refer=http%3A%2F%2Fc-ssl.duitang.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1618585712&t=fbe636ae0e127e82222af8c0c1f30ec8)  
这是统计学的基本概念，随便找本概率论基础都可以找到这些概念，看不懂的就看多几遍，重点在记住和知道应用场合，**知识点之间的衔接很重要**，理解为王。  

>
### 1. 随机变量分类
离散型随机变量和连续型随机变量。  
区别点在于：结果数量**是否可数**。  
举例子，每天浏览A网站的人数是离散型随机变量，它可能是500,5000,50000个，绝不可能是5.5个这样的不可数情况。  
B网站对“付款”这个按钮做了颜色更改，现在想知道产品的付款转化率，这个值在[0,1]区间里，这是连续型随机变量。  

### 2. 常见的离散分布
#### 2.1 伯努利分布(0-1分布)
试验的结果只有两种，0表示不发生，1表示发生。用p表示事件1发生的概率，1-p表示事件0发生的概率。  
经典例子，抛硬币。还比如，买彩票中不中奖，走在路上捡不捡得到100万……
#### 2.2 二项分布
n个重复独立的伯努利分布就是二项分布(也叫n重伯努利分布)。
重复独立：
(1)每个伯努利分布事件发生的概率均为p
(2)每个试验的结果相互独立，不受干扰
例子：  
抛n次硬币  
买n次彩票  
走n次路

最终有x次抛到正面的概率、有x次中彩票的概率、有x次捡到100万的概率都可用下列式子表示：
:smiling_imp:

#### 2.3 泊松分布
描述在单位时间(空间)内随机事件发生的次数。  
比如，A网站在今天9:00-10:00内的UV、PV；  
珠江新城站在今天早上7:00-9:00的进站人数。  
## 分布律欠着

### 3. 常见的连续分布
这里会涉及CDF和PDF，即累积分布函数F(x)和概率密度函数f(x)。
:smiling_imp:  
3.1 均匀分布  
3.2 正态分布 
3.3 指数分布  

大学考试的时候经常考，出来后建模也经常遇到，特别是正态分布，好好记。

### 4. 描述随机变量的数学特征
#### 4.1 期望，E(X)
全名：数学期望，表示随机变量X的平均水平，记作E(X)。
将X所对应的随机变量重复多次，随着试验次数的增加，X的均值<img src="https://latex.codecogs.com/gif.latex?\bar{x}" title="\bar{x}" />会愈发趋近于E(X)。  

对于离散型随机变量，基于分布律P（X=x），<img src="https://latex.codecogs.com/gif.latex?E(X)=\sum(xP(X=x))" title="E(X)=\sum(xP(X=x))" />;  
对于连续型随机变量，基于概率密度函数f（x），<img src="https://latex.codecogs.com/gif.latex?E(X)=\int_{-\infty}^{&plus;\infty}xf(x)dx" title="E(X)=\int_{-\infty}^{+\infty}xf(x)dx" />

#### 4.2 方差, D(X) or Var(X)
方差刻画随机变量X的波动大小，记作D(X) or Var(X)。D(X)=E(X-E(X))^2

#### 4.3 标准差, <img src="https://latex.codecogs.com/gif.latex?\sigma(X)" title="\sigma(X)" />
标准差就是方差开根号
<img src="https://latex.codecogs.com/gif.latex?\sigma(X)=\sqrt[]{D(X)}" title="\sigma(X)=\sqrt[]{D(X)}" />
注：  
当知道一个随机变量的数学期望和标准差后，可以对它进行一个标准化处理：  
<img src="https://latex.codecogs.com/gif.latex?X'=\frac{(X-\mu)}{\sigma&space;}" title="X'=\frac{(X-\mu)}{\sigma }" />

#### 4.3 分位数
中位数，上四分位数，下四分位数等  
画箱线图的时候会涉及，一般用来看数据的一个分布情况，看是否有远离大部队的异常值

#### 4.4 协方差和相关系数
上面是单个随机变量的特征，而这两个指标关注的是两个或者多个随机变量之间的关系。   
假设有两个随机变量X、Y，
协方差：Cov(X,Y)=E(X-E(X))(Y-E(Y))  
相关系数：<img src="https://latex.codecogs.com/gif.latex?\rho_{xy}=\frac{Cov(X,Y)}{\sigma(X)\sigma(Y)}" title="\rho_{xy}=\frac{Cov(X,Y)}{\sigma(X)\sigma(Y)}" />

方差是协方差的一种特殊情况，当两个随机变量一样时，协方差就变成方差公式了。  
- 怎么理解协方差呢？  
> 直接理解成：两个随机变量在变化过程中是同方向变化？还是反方向变化？同向或反向程度如何？  
> 再形象点，随机变量X变大，随机变量Y也变大，两者同向发展，那么协方差为正。而且协方差越大，他们同向的程度越强。  

- 相关系数：描述的是X和Y是否存在线性相关关系。一定要记住是线性关系，即使相关系数为0，不代表两个随机变量完全没有关系，因为可能存在非线性关系呢！    
- 当两个随机变量相互独立，协方差和相关系数都会等于0。但是，反过来说，就不一定成立啦。  

#### 4.5 拓展
- 常见分布的数学期望、方差最好看书记一下，如果要挖这块理论的话，记住很有作用。  
- 说说独立与不相关：
> 独立就绝对不相关，但是不相关就不代表独立。因为相关仅仅描述线性关系，还有非线性关系没有考虑。所以，不相关的范围会更大一点，它包含独立，也包含不独立。  
用图片理解一下：  
![](https://ae02.alicdn.com/kf/U43aa769c15954d9ca0eac43efc2f9852J.jpg)

### 5. 正态分布  
这是连续型随机变量的一种分布。又叫正常分布，高斯分布。正态分布是许多统计方法的理论基础，如果不满足正态分布这个条件，很多方法用不了。  
其概率密度函数图形呈中间高，两边低，以期望为中心左右对称分布。(钟形)    
就此引出<img src="https://latex.codecogs.com/gif.latex?3\sigma" title="3\sigma" />原则：
图形以数学期望为中心对称，样本落在：  
:smiling_imp:[<img src="https://latex.codecogs.com/gif.latex?\mu&space;-\sigma" title="\mu -\sigma" />,<img src="https://latex.codecogs.com/gif.latex?\mu&plus;\sigma" title="\mu+\sigma" />]
概率是68.27%  
:smiling_imp:[]的概率是95.45%  
:smiling_imp:[]的概率是99.73%  
样本落在<img src="https://latex.codecogs.com/gif.latex?3\sigma" title="3\sigma" />之外的概率只有0.27%，这部分误差不再属于随机误差，而是粗大误差，应该将这部分数据予以剔除。  

### 6.大数定律
核心点：随机变量X所对应的随机试验重复多次，随着试验次数的增加，X的均值<img src="https://latex.codecogs.com/gif.latex?\bar{x}" title="\bar{x}" />会愈发趋于E(X).  
大数定律分几种：辛钦大数定律、伯努利大数定律、切比雪夫大数定律  
|  定律   | 分布情况  |  期望   | 方差  |  总结   |
|  ----  | ----  |----  |----  |----  |
| 辛钦大数定律  | 相互独立且同分布 | 相同  | 相同| 估算期望|
| 伯努利大数定律  | 二项分布 | 相同  | 相同| 频率等于概率|
| 切比雪夫大数定律  | 相互独立或不相关 | 存在  | 存在| 估算期望|

:smiling_imp:**条件最宽松的是切比雪夫大数定律，应用也相对广泛，后面再补具体的定义。**

### 7. 中心极限定理
给定一个任意分布的总体，从这个总体中随机抽取 n 个数据（一般认为，每组大于等于30个），一共抽 m 组。 然后求 m 个平均值，这些平均值的分布接近正态分布。  
在AB测试中，会基于这一块知识做出假设检验，需要熟记，面试经常会问。  
如果理解不了，可以用图辅助理解，随着样本量增大，图形越趋于正态分布。    

### 8. 假设检验  
先假设后检验。应用场景很多，比如，某APP上线了新模块，你想看新版本是否比老版本好；新推出一种药，想知道患者吃了有没用；厂家推出新的灯泡，商家想知道这批货跟上一批货质量有么差别……  
于是，要根据你的立场和利益，去做假设，然后利用样本信息去检验假设是否正确。  

#### 8.1 原假设和备择假设 
原假设记为H0,备择假设记为H1。我们真正关心和证明的是备择假设，一般先确定备择假设，再确定原假设。  
没有明确规定谁做原假设、备择假设，需要根据问题的情况，去定义。 
注:  
- 原假设与备择假设相互排斥，两者中有且仅有一个正确。  
- 一般研究者希望原假设能够被拒绝，备择假设能够被接受，但若没有充分理由证明原假设错误，就不能够轻易拒绝原假设。需要注意，我们并不是简单地根据样本结果来判断原假设与备择假设哪个更正确，这两个假设并不能同等看待。在假设检验中，原假设受到保护，不能够被轻易拒绝，处于有利地位。另一方面就算原假设被接受，也仅能说明拒绝它的理由不够充分，而不意味着原假设必然正确。

后面会涉及一个概念：检验统计量。  
这是用于假设检验计算的统计量，基于样本检验统计量的值来接受或者拒绝原假设。理解起来，就是这是个在假设成立下的理论值。后面，代入样本算一个实际值，拿理论值和实际值做比较，看假设是否成立。     

#### 8.2 假设检验的基本思想
通过证明在原假设成立的前提下，检验统计量出现当前值或者更为极端的值属于小概率事件，以此推翻原假设，选择备择假设。    
上面所说的“检验统计量出现当前值或者更为极端的值”的概率就是常说的P值。当p值与预先设定的显著性水平<img src="https://latex.codecogs.com/gif.latex?\alpha" title="\alpha" />进行对比，若p<<img src="https://latex.codecogs.com/gif.latex?\alpha" title="\alpha" />,就推翻原假设(用图理解，若p<<img src="https://latex.codecogs.com/gif.latex?\alpha" title="\alpha" />,就意味着落在总体分布的更外围地方，就是落入拒绝域，所以要拒绝原假设。一种用检验统计量的值来理解，一种用P值来理解，最好辅助图理解教材)  

#### 8.3 两类错误
第一类错误：原假设成立情况下，拒绝了原假设(速记：拒真)    
第二类错误：原假设不成立的情况下，接受了原假设(速记：采伪)        
这两类错误都无法规避，因为实际上，你并不知道实际情况，一切都是在假设。    

#### 8.4 如何平衡两类错误
实验的结果最好是两类错误的犯错率都很低，但是很不巧，现实中一个值小另一个值就会大，一个值大另一个值就会小。到底要控制哪一类犯错率呢？后来，统计学家考虑到定一类错误会相对比较好证明，于是后来一般都定一型错误，再在显著性水平固定的情况下，去减少第二类错误发生的概率。      

#### 8.5 显著性水平和置信度    
显著性水平：表示在假设检验中，犯一类错误的上限，用<img src="https://latex.codecogs.com/gif.latex?\alpha" title="\alpha" />表示。           
置信度：表示检验的置信度，用<img src="https://latex.codecogs.com/gif.latex?1-\alpha" title="1-\alpha" />表示。      
一般，设置显著性水平为0.05，如果要求更严格，可设0.01，这都取决于你业务想要的严谨度。   

#### 8.6 检验效能  
在显著性水平固定的情况下，需要避免第二类错误的概率。<img src="https://latex.codecogs.com/gif.latex?1-\beta" title="1-\beta" />即规避二类错误的概率，用power表示。    
power的大小可以通过增加样本量来提高，通常需要power达到80%或以上。(<img src="https://latex.codecogs.com/gif.latex?\beta" title="\beta" />为犯二类错误的概率) 

#### 8.7 最小样本量  
通过预先设定的显著性水平和检验效能，可以计算出试验所需要的最小样本量。    
AB测试中会用到，当显著性水平越低、检验效能越高，样本量就越要大。     

#### 8.8 P 值
在原假设成立的前提下，检验统计量出现当前值或者更为极端的值的概率。  

### 9. 贝叶斯统计
:smiling_imp:很久没碰这块了，脑子转不过来，后面补。


