![](https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fc-ssl.duitang.com%2Fuploads%2Fitem%2F201908%2F09%2F20190809222848_dvxth.thumb.400_0.jpg&refer=http%3A%2F%2Fc-ssl.duitang.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1618585712&t=fbe636ae0e127e82222af8c0c1f30ec8)  
这是统计学的基本概念，随便找本概率论基础都可以找到这些概念，看不懂的就看多几遍，重点在记住和知道应用场合，单纯背没什么意义。  
>
### 1. 随机变量分类
离散型随机变量和连续型随机变量。  
区别点在于：结果数量**是否可数**。  
举例子，每天浏览A网站的人数是离散型随机变量，它可能是500,5000,50000个，绝不可能是5.5个这样的不可数情况。  
B网站对“付款”这个按钮做了颜色更改，现在想知道产品的付款转化率，这个值在[0,1]区间里，这是连续型随机变量。  

### 2. 常见的离散分布
#### 2.1 伯努利分布(0-1分布)
试验的结果只有两种，0表示不发生，1表示发生。用p表示事件1发生的概率，1-p表示事件0发生的概率。  
经典例子，抛硬币。还比如，买彩票中不中奖，走在路上捡不捡得到100万……
#### 2.2 二项分布
n个重复独立的伯努利分布就是二项分布(也叫n重伯努利分布)。
重复独立：
(1)每个伯努利分布事件发生的概率均为p
(2)每个试验的结果相互独立，不受干扰
例子：  
抛n次硬币  
买n次彩票  
走n次路

最终有x次抛到正面的概率、有x次中彩票的概率、有x次捡到100万的概率都可用下列式子表示：
:smiling_imp:

#### 2.3 泊松分布
描述在单位时间(空间)内随机事件发生的次数。  
比如，A网站在今天9:00-10:00内的UV、PV；  
珠江新城站在今天早上7:00-9:00的进站人数。  
## 分布律欠着

### 3. 常见的连续分布
这里会涉及CDF和PDF，即累积分布函数F(x)和概率密度函数f(x)。
:smiling_imp:  
3.1 均匀分布  
3.2 正态分布  
3.3 指数分布  

大学考试的时候经常考，出来后建模也经常遇到，特别是正态分布，好好记。

### 4. 描述随机变量的数学特征
#### 4.1 期望，E(X)
全名：数学期望，表示随机变量X的平均水平，记作E(X)。
将X所对应的随机变量重复多次，随着试验次数的增加，X的均值<img src="https://latex.codecogs.com/gif.latex?\bar{x}" title="\bar{x}" />会愈发趋近于E(X)。  

对于离散型随机变量，基于分布律P（X=x），<img src="https://latex.codecogs.com/gif.latex?E(X)=\sum(xP(X=x))" title="E(X)=\sum(xP(X=x))" />;  
对于连续型随机变量，基于概率密度函数f（x），<img src="https://latex.codecogs.com/gif.latex?E(X)=\int_{-\infty}^{&plus;\infty}xf(x)dx" title="E(X)=\int_{-\infty}^{+\infty}xf(x)dx" />

#### 4.2 方差, D(X) or Var(X)
方差刻画随机变量X的波动大小，记作D(X) or Var(X)。D(X)=E(X-E(X))^2

#### 4.3 标准差, <img src="https://latex.codecogs.com/gif.latex?\sigma(X)" title="\sigma(X)" />
标准差就是方差开根号
<img src="https://latex.codecogs.com/gif.latex?\sigma(X)=\sqrt[]{D(X)}" title="\sigma(X)=\sqrt[]{D(X)}" />
注：  
当知道一个随机变量的数学期望和标准差后，可以对它进行一个标准化处理：  
<img src="https://latex.codecogs.com/gif.latex?X'=\frac{(X-\mu)}{\sigma&space;}" title="X'=\frac{(X-\mu)}{\sigma }" />

#### 4.3 分位数
中位数，上四分位数，下四分位数等  
画箱线图的时候会涉及，一般用来看数据的一个分布情况，看是否有远离大部队的异常值

#### 4.4 协方差和相关系数
上面是单个随机变量的特征，而这两个指标关注的是两个或者多个随机变量之间的关系。   
假设有两个随机变量X、Y，
协方差：Cov(X,Y)=E(X-E(X))(Y-E(Y))  
相关系数：<img src="https://latex.codecogs.com/gif.latex?\rho_{xy}=\frac{Cov(X,Y)}{\sigma(X)\sigma(Y)}" title="\rho_{xy}=\frac{Cov(X,Y)}{\sigma(X)\sigma(Y)}" />

方差是协方差的一种特殊情况，当两个随机变量一样时，协方差就变成方差公式了。  
- 怎么理解协方差呢？  
> 直接理解成：两个随机变量在变化过程中是同方向变化？还是反方向变化？同向或反向程度如何？  
> 再形象点，随机变量X变大，随机变量Y也变大，两者同向发展，那么协方差为正。而且协方差越大，他们同向的程度越强。  

- 相关系数：描述的是X和Y是否存在线性相关关系。一定要记住是线性关系，即使相关系数为0，不代表两个随机变量完全没有关系，因为可能存在非线性关系呢！    
- 当两个随机变量相互独立，协方差和相关系数都会等于0。但是，反过来说，就不一定成立啦。  

#### 4.5 拓展
- 常见分布的数学期望、方差最好看书记一下，如果要挖这块理论的话，记住很有作用。  
- 说说独立与不相关：
> 独立就绝对不相关，但是不相关就不代表独立。因为相关仅仅描述线性关系，还有非线性关系没有考虑。所以，不相关的范围会更大一点，它包含独立，也包含不独立。  
用图片理解一下：  
![](https://ae02.alicdn.com/kf/U43aa769c15954d9ca0eac43efc2f9852J.jpg)

### 5. 正态分布  
这是连续型随机变量的一种分布。又叫正常分布，高斯分布。正态分布是许多统计方法的理论基础，如果不满足正态分布这个条件，很多方法用不了。  
其概率密度函数图形呈中间高，两边低，以期望为中心左右对称分布。(钟形)    
就此引出<img src="https://latex.codecogs.com/gif.latex?3\sigma" title="3\sigma" />原则：
图形以数学期望为中心对称，样本落在：  
:smiling_imp:[<img src="https://latex.codecogs.com/gif.latex?\mu&space;-\sigma" title="\mu -\sigma" />,<img src="https://latex.codecogs.com/gif.latex?\mu&plus;\sigma" title="\mu+\sigma" />]
概率是68.27%  
:smiling_imp:[]的概率是95.45%  
:smiling_imp:[]的概率是99.73%  
样本落在<img src="https://latex.codecogs.com/gif.latex?3\sigma" title="3\sigma" />之外的概率只有0.27%，这部分误差不再属于随机误差，而是粗大误差，应该将这部分数据予以剔除。  

### 6.大数定律
核心点：随机变量X所对应的随机试验重复多次，随着试验次数的增加，X的均值<img src="https://latex.codecogs.com/gif.latex?\bar{x}" title="\bar{x}" />会愈发趋于E(X).  
大数定律分几种：辛钦大数定律、伯努利大数定律、切比雪夫大数定律  
|  定律   | 分布情况  |  期望   | 方差  |  总结   |
|  ----  | ----  |----  |----  |----  |
| 辛钦大数定律  | 相互独立且同分布 | 相同  | 相同| 估算期望|
| 伯努利大数定律  | 二项分布 | 相同  | 相同| 频率等于概率|
| 切比雪夫大数定律  | 相互独立或不相关 | 存在  | 存在| 估算期望|
:smiling_imp:**条件最宽松的是切比雪夫大数定律，应用也相对广泛，后面再补具体的定义。**

### 7. 中心极限定理
给定一个任意分布的总体，从这个总体中随机抽取 n 个数据（一般认为，每组大于等于30个），一共抽 m 组。 然后求 m 个平均值，这些平均值的分布接近正态分布。  
在AB测试中，会经常用到这一块知识，需要熟记。  
如果理解不了，可以用图辅助理解，随着样本量增大，图形越趋于正态分布。    

### 8. 假设检验  
比如，某APP上线了新功能，你想看是否有正面影响，于是就要用到假设检验啦。  
#### 8.1 原假设和备择假设 
原假设记为H0,备择假设记为H1。我们真正关心和证明的是备择假设，一般先确定备择假设，再确定原假设。  
没有明确规定谁做原假设、备择假设，需要根据问题的情况，去定义。  

后面会涉及一个概念：检验统计量。  
这是用于假设检验计算的统计量，基于样本检验统计量的值来接受或者拒绝原假设。  

#### 8.2 假设检验的基本思想
通过证明在原假设成立的前提下，检验统计量出现当前值或者更为极端的值属于小概率事件，以此推翻原假设，选择备择假设。    
上面所说的“检验统计量出现当前值或者更为极端的值”的概率就是常说的P值。当p值与预先设定的显著性水平<img src="https://latex.codecogs.com/gif.latex?\alpha" title="\alpha" />进行对比，若p<<img src="https://latex.codecogs.com/gif.latex?\alpha" title="\alpha" />,就推翻原假设(用图理解，若p<<img src="https://latex.codecogs.com/gif.latex?\alpha" title="\alpha" />,就意味着落在总体分布的更外围地方，就是落入拒绝域，所以要拒绝原假设。一种用检验统计量的值来理解，一种用P值来理解，最好辅助图理解教材)  

#### 8.3 两类错误
第一类错误：原假设成立情况下，拒绝了原假设
第二类错误：原假设不成立的情况下，接受了原假设  
速记叫**“拒真采伪”**

#### 8.4 如何平衡两类错误
实验的结果最好是两类错误的概率都低，但是很不巧，一个小另一个就会大，一个大另一个就会小。后来，考虑到定一型错误会相对比较好证明，我们一般都定显著性水平<img src="https://latex.codecogs.com/gif.latex?\alpha" title="\alpha" />先，再去考虑<img src="https://latex.codecogs.com/gif.latex?\beta" title="\beta" />的情况。  
通常情况下，会预先设定犯第一类错误的上限：显著性水平<img src="https://latex.codecogs.com/gif.latex?\alpha" title="\alpha" />，

在原假设成立时，检验统计量会服从一个特定的分布；而在备择假设成立时，则不服从分布。  
