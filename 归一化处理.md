背景：  
在特征工程中，经常会看到归一化处理，有的是最大-最小化处理，有的是标准化处理……   
为什么要做归一化处理呢？直接用数据不行吗？    

- 目录：  
    - 1.为什么要做归一化处理？  
    - 2.常见的归一化方法  
    - 3.不同方法的联系与区别，应用场景
    - 4.python用法
--------

#### 1.为什么要做归一化处理？  
- (1) 消除特征间量纲/单位/尺度差异的影响，让特征有同等地位，提高建模效果。现实数据中，各特征的尺度/量纲/单位可能不同。比如身高和体重，两者无法比较衡量；
比如有的特征变化范围可能是\[1000,100000]，有的特征变化范围是\[1,100]。可能后一类的特征是比较重要的，但是模型出来后，反而前一类特征起决定性作用。  
- (2) 更快、更顺利找到模型最优解。在建模的时候，我们想要找到最优解，也就是损失函数最小时对应的特征系数。在归一化前，寻找最优解的过程相对曲折、困难。  
而归一化后，寻找最优解的路径可能会相对平稳、顺利。
- (3) 对于基于距离的挖掘算法而言，归一化尤为重要。  

#### 2.常见的归一化方法  
- (1) 归一化（normalization）  
一般都用 最大最小归一化：![](http://latex.codecogs.com/gif.latex?\\frac{X-x_{min}}{x_{max}-x_{min}})  
最大最小归一化跟样本最大最小值的关系很大。特征将按原始数据的等比例缩放，映射到\[0,1]之间。  
- (2) 标准化  
一般用的是z score标准化：![](http://latex.codecogs.com/gif.latex?\\frac{X-\mu}{\sigma})   
z score标准化与样本的均值、标准差有关系。处理后的数据的均值为0，方差为1（但是，不一定是标准正态分布，要看偏度、峰度和概率密度函数；如果原先是正态分布，转化后就是标准正态分布，但是如果原来不是正态分布，标准化后也不是正态分布。）  

#### 3.不同方法的联系与区别，应用场景
- (1) 联系   
两者本质上都是对特征进行线性变换。而线性变换不改变原始数据的数值排序，大的还是大，小的还是小，变换后的数据对建模是有意义的。
- (2) 区别  
    - (2.1) 通过另个式子，可以发现最大最小归一化输出范围为\[0,1]，主要由特征的最大、最小值决定数据的变换。如果出现异常值（特别大），其他数据集中于比较小的地方，归一化后数据可能存在一大段空白的缺口，就会影响建模。其次，如果新数据超出了建模时的\[min,max]，则要重新定义min和max。  
    - (2.2) z score标准化输出范围是负无穷到正无穷。其输出与样本均值和样本标准差有关，而样本均值与样本标准差与每个样本都有关，和整体的分布有关，并不单单由某几个值确定。  
- (3) 应用场景    
    - (3.1) 数据较为稳定，没有很大或很小的异常值，可用归一化  
    - (3.2) 输出结果范围有要求，可用归一化
    - (3.3) 如果数据存在影响值和噪音，用z score标准化更佳，它可以减小这些值带来影响。因为分母是标准差，当标准差小，就是数据分布比较集中，标准化后数据变大，更散了。而当标准差大，就是数据分布比较散，标准化后数据变小，更集中了。所以它比归一化更适合于处理异常值。
    - (3.4) 涉及或隐含距离计算的算法，比如Kmeans、KNN、PCA、SVM、Naive Bayes、adaboost、gbdt、xgboost、神经网络等，一般需要归一化。而关心变量的分布和变量之间的条件概率，如决策树、随机森林,不需要归一化。   
    - (3.5) 上大学的时候用的比较多的是z score标准化，这个整体相对稳一点。但是具体要选哪种，需看需求。  

#### 4.python用法 
- sklearn.preprocessing.StandardScaler()  缩放到均值为0，方差为1。若有异常值，最好用robust
- sklearn.preprocessing.MinMaxScaler()  缩放到0和1之间,若新数据集最大最小值范围有变，需重新定义minmax_scale
- sklearn.preprocessing.MaxAbsScaler()  缩放到-1和1之间，跟minmax很像，这个为稀疏数据而生
- sklearn.preprocessing.RobustScaler()  为异常值而生
- sklearn.preprocessing.preprocessing.Normalizer()  文本分类or聚类时常用，默认对样本正则化，保留原始数据的分布
可参阅官方文档：  
https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py
