![](https://ss1.bdstatic.com/70cFuXSh_Q1YnxGkpoWK1HF6hhy/it/u=3429810035,2951434864&fm=26&gp=0.jpg)    

### 目录
- 定义
- 划分选择
- 剪枝
- 算法优缺点
- 调优策略

### 一. 定义：  
决策树是一种非参数模型(不像回归，有个基本形式，这更像一个黑盒)，它无须对目标函数和变量做过多的假设，使用更加灵活，能够处理更加复杂场景下的问题。它是一种类似于流程图的树形结构，树内部的每一个节点代表对一个特征的测试，树的分支代表该特征的每一个测试结果，树的每一个叶子节点代表一个类别(决策结果)。    
一般，一棵决策树包含一个根节点、若干个内部节点和若干个叶节点。  
决策树学习的目的是为了产生一棵泛化能力强，即处理未见示例能力强的决策树。  

### 二. 划分选择：
前提知识：信息熵：事件的不确定性，是度量样本集合纯度最常用的一种指标。可以理解为：当某件事很复杂，有多种决策情况，熵就大；当事件很简单，决策单一，熵就小。
ID3和C4.5对于特征的选取都是基于信息熵的，在不断生成新节点的时候，熵也在不断降低。   
  
下面是三种划分依据：   
#### 2.1 ID3:  
用**信息增益**来选择合适的特征(划分属性)作为节点。  
当信息增益越大，就意味着使用该属性来进行划分所获得的纯度提升越大，thus，优先选这个属性。    
例子见西瓜书，很容易理解。  
缺点：信息增益会偏向选择取值数目较多的属性。  

#### 2.2 C4.5:  
用**信息增益和信息增益率**来选择合适的特征作为节点。单单用信息增益率的话，会偏向于选择数目较少的特征。所以，算法先从信息增益率高于平均水平的属性，再从中选择增益率最高的特征。   

#### 2.3 CART:  
使用**基尼指数**(Gini index)来选择划分属性。  
基尼指数越小，数据集的纯度越高。选择使得划分后基尼指数最小的属性作为最优划分属性。  

说明：  
ID3、C4.5只能处理分类问题，CART可处理分类和回归问题。
ID3不可处理连续值，C4.5和CART可以处理连续值，但是C4.5实现会比较复杂。   
在实际工作中，CART的应用更广泛，sklearn中默认用这个。  

### 三.剪枝
分预剪枝和后剪枝   
后补（）


### 四. 算法优缺点：
#### 4.1 优点
- 决策树属于非参数模型，相比于线性回归模型和逻辑回归模型，不需要对样本进行预先假设，所以可以处理更为复杂的样本
- 简单直观，具有很强的可解释性。通过绘制分支，可知道整个流程，快速发现影响最终结果的因素，方便业务人员做调整
- 不需要做归一化处理、处理缺失值
- 可处理离散值也可以连续值
- 对缺失值不敏感，容错能力较强

#### 4.2 缺点
- 易过拟合
- 易因样本的轻微变动，影响树结构的剧烈改变(可用集成学习去解决)
- 在处理特征关联性比较强的数据时，表现不是很好

### 五. 调优策略
- 控制树的深度及节点的个数等参数，避免过拟合(剪枝)
- 运用交叉验证，调参
- 基于决策树，用集成方法，让模型更具鲁棒性


参考资料：  
https://www.cnblogs.com/pinard/p/6053344.html  
西瓜书  

