## Adaboost
回归Boosting：Boosting方法训练基学习器时采用串行的方式，各个基学习器之间有依赖。  
它的基本思想是将基学习器层层叠加，每一层在训练的时候，对前一层基学习器分错的样本，给予更高的权重。测试时，根据各层分类器的结果的加权得到最终结果。这是一个不断迭代的过程，可以看作：Learn from your mistake.

### 一. Adaboost 在 Boosting 思想的基础上，在两方面做出了改变：  
- (1) 每一轮如何改变训练数据的权值或概率分布  
> 提高前一轮被弱学习器错误分类样本的权值，而降低被正确分类样本的权值(分类错误的样本在后续会得到更多的关注)   
- (2) 如何将弱学习器组合成一个强分类器  
> 对于结合策略，Adaboost采用加权多数表决法，即加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用，减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用。  
> 

### 二. 具体步骤：  
给定数据集 T={(x1,y1), (x2,y2) , … , (xn,yn)}, 最终分类器为 G(x)。  
1. 初始化样本权重 1/n，权值分布构成权值向量 D1 = {w11, w12 ,…, w1n}。开始时，每个样本的权重都应该是一样的；  
2. 在上述样本概率分布情况下，训练弱分类器 G1；    
3. 计算弱分类器 G(1) 的分类误差率 e1；  
4. 计算弱分类器 G(1) 的系数 α1(在结合策略里用到的权重)，该指标是弱分类器的重要度指标，分类误差率越小的基本分类器在最终分类器中的作用越大；  
5. 更新训练样本集的权重分布 D2。被错误分类的样本权重变大，被正确分类的样本权重变小，使误分类样本在下一轮学习中起更大的作用。不改变所给的训练数据，而不断改变训练数据权值的分布，是训练数据在基本分类器的学习中起不同的作用，这是Adaboost算法的一大特点； 
6. 后续步骤，不断重复上述2-5步骤，直至迭代次数达到我们指定的某个值 T。  
7. 构建线性组合形成强学习器 G(x)
小；

### 三. Adaboost算法的另一种解释：  
模型是加法模型、损失函数为指数函数、学习算法为前向分布算法时的二分类学习算法。  

