## 基于密度的聚类算法DBSCAN(Density-Based Spatial Clustering of Applications with Noise)
本文介绍基于密度的聚类算法，这类算法假定聚类结构能通过样本分布的紧密程度确定。  
通常情况下，密度聚类算法从样本密度的角度来考察样本之间的可连接性，并给予可连接样本不断扩展聚类簇以获得最终的聚类结果。  
>
### 一. DBSCAN基础概念及原理步骤：  
DBSCAN(具有噪声的基于密度的聚类方法)是其中一种基于密度的聚类算法,  它基于一组邻域参数参数(ϵ, MinPts)来刻画样本分布的紧密程度。
#### (1.1) 基本概念：  
给定数据集D={x1, x2, x3, ……, xm},
- ϵ-邻域：对于xj∈D，其ϵ-邻域包含样本集D中与xj的距离不大于ϵ的子样本集，即Nϵ(xj)={xi∈D|dist(xi,xj)≤ϵ}。定义点xj领域里的其他其他点的个数记为|Nϵ(xj)|　(注：默认选用欧式距离来作为dist()函数)   
- 核心对象：对于任一样本xj∈D，如果其ϵ-邻域对应的Nϵ(xj)至少包含MinPts个样本，即如果|Nϵ(xj)|≥MinPts，则xj是核心对象。　 
(可以将ϵ理解为半径，MinPts理解为点周围的点个数。以某个点为中心画个圆，如果圈里面的数量达标，它就是核心对象)
- 密度直达：如果xj位于xi的ϵ-邻域中，且xi是核心对象，则称xj由xi密度直达。(注意位置，反过来说不一定对）  
- 密度可达：对于xi和xj, 若存在样本样本序列p1,p2,...,pn,其中，p1=xi,pT=xj, 且pt+1由pt密度直达，则称xj由xi密度可达。也就是说，密度可达满足传递性。此时序列中的传递样本p1,p2,...,pT−1均为核心对象，因为只有核心对象才能使其他样本密度直达。注意密度可达也不满足对称性，这个可以由密度直达的不对称性得出。
- 密度相连：对于xi和xj,如果存在核心对象样本xk，使xi和xj均由xk密度可达，则称xi和xj密度相连。注意密度相连关系是满足对称性的。

从下图可以很容易看出理解上述定义，图中MinPts=5，红色的点都是核心对象，因为其ϵ-邻域至少有5个样本。黑色的样本是非核心对象。所有核心对象密度直达的样本在以红色核心对象为中心的超球体内，如果不在超球体内，则不能密度直达。图中用绿色箭头连起来的核心对象组成了密度可达的样本序列。在这些密度可达的样本序列的ϵ-邻域内所有的样本相互都是密度相连的。

这是一个循环迭代的算法，简单易懂。假定我们要对 N 个样本观测做聚类：  
(1) 假设聚为 K 类，并且随机选择 K 个点作为初始中心点；  
(2) 接下来，按照距离初始中心点最小的原则，把所有观测分到各中心点所在的类中；  
(3) 每类中有若干个观测，计算 K 个类中所有样本点的均值，作为第二次迭代的 K 个中心点；  
(4) 然后根据这个中心重复第 2、3 步，直到收敛（中心点不再改变或达到指定的迭代次数），聚类过程结束。  
下图展示最简单的聚类，分 2 类：  
![](https://ftp.bmp.ovh/imgs/2020/12/12cb6a37de432746.png)  
实际情况中，质心一般要迭代多次，才能达到相对最优情况。
可以以传销的方式去理解整个过程：
(1) 首先，在整个数据集里随机地选取某个点A，然后根据我们设定的标准（半径以及圈内数量），评判A是否能开展一个新簇。假设半径为2、最小数量为4，如果点A达到以上标准（满足条件，A就是一个核心对象），那么可以以它为基础开展一个新簇。  
(2) 假设点A圈住了5个点：B C D E F，现在这5个点就要去发展下线了。同样是以上述标准，去看他们能否发展下线。  
(3) 不断重复步骤（2），直至没有符合标准的下线了。这就构成了第一个簇C1。  
(4) 除了C1中的点，随机选取某个点，看它能否像点A一样去发展一个新簇，重复（1）（2）（3）步骤，不断去找新簇。  
(5) 直到找不到新簇了，没纳进新簇的就是异常值点。    
### 二. 算法优缺点：
#### (2.1) 优点：  
- 不需要事先给定一个K值（KMmeans需要事先确定一个K值）    
- 适用于任意形状的稠密数据集，不单单是凸数据集（KMmeans仅对凸数据集聚类效果好）    
- 能够有效的发现噪声点、异常值，可用于异常值检测
- 初始值的选择对聚类结果无影响，聚类结果几乎不依赖于结点遍历顺序，没有偏倚（初始质心的选择影响KMmeans迭代速度及效果）
>
#### (2.2) 缺点：  
- 当聚类的密度不同或或类间的距离相差很大时，DBSCAN的性能会不如其他算法   
- 算法涉及两个参数：距离阈值ϵ，邻域样本数阈值MinPts。需要联合调参，不同的参数组合对最后的聚类效果有较大影响
- 面对高维数据容易溢出，算起来慢（可以先做降维）
- 该算法的运行速度要比 KMeans 算法慢一些
>
### 三. k值确定：
#### (3.1) 个人需求/经验  
比如，做客户分层，你想分成4部分，那么k值就取你想要的4。
>
#### (3.2) 手肘法Elbow method——看图辨别拐点   

- (3.2.1) SSE参数解释：  
得很缓慢时，就认为进一步增大聚类数效果也聚类效果也没有太明显的变化。关注斜率最大处，一个明显的“肘点”就是最佳聚类数目。
- (3.2.3) 举例SSE图表示：
![](https://pic2.zhimg.com/v2-25b396108e9b5da6094c2097888f2251_b.png)   
此时，选k=3。  
>
#### (3.3) Gap statistic   

>
#### (3.4) 轮廓系数Silhouette Coefficient

>
#### (3.5) Calinski-Harabasz Index 即(CH)指标  

>
### 四. 举栗子：
下面用sklearn的make_blobs方法来生成聚类算法的测试数据：  

![](https://ftp.bmp.ovh/imgs/2020/12/169871e487d9afb4.png)
(4.1) 手肘法



参考资料：  
西瓜书
[B站视频](https://www.bilibili.com/video/BV1j4411H7xv?p=1)  
大佬Blog,[(1)](https://www.cnblogs.com/pinard/p/6208966.html)、知乎多个回答
聚类效果可视化体验(某位外国大佬写的体验网站)[地址](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/)  
![](https://ftp.bmp.ovh/imgs/2020/12/1187054a9d252826.png)  
