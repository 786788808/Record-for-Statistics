# 主成分分析法 PCA (Principal components analysis)
提到 PCA，第一反应就是做降维。  
对这块涉及的很多数学概念遗忘了，该篇来整理一下思路，涉及的数学知识后面再慢慢补。不做单纯的公式推断，因为很容易忘。  

### 目录： 
- PCA 概念
- 为什么要做PCA
- 算法优缺点

### 一. PCA 概念
(假设现有数据集：m 行 n 列)
PCA, 主成分分析法，一种应用广泛的数据降维算法(无监督)，也可以用于提取数据的主要特征。它通过构建新坐标轴，将原有的 n 维数据降成新的 n' 维(n'<n, n'维特征叫做主成分，且正交，也就是互不相关)。    
实现步骤：  
PCA要找一组相互正交的坐标轴，来表示原有的特征。算法依次找到合适的轴，首先，第一个轴方向会选择原始数据中方差最大的方向；接着，第二个轴选取，需要与第一个轴正交的平面中
构建新的主成分(维度较小)来代替原有的高维数据。该过程会有信息丢失问题，但能极大缓和高维带来的维度灾难系列问题。（常见的还有LDA算法，但那是监督算法）  
假设，现有数据集m 行 n 列，数据量较大，如果直接建模耗时长，而且效果不一定好。可以考虑PCA，它可通过旋转坐标轴，找到一个新的超平面，让样本点到这个超平面的距离足够近，或者说样本点在这个超平面的投影尽可能地分开。原有的数据用新的特征来表示，从高维 n 降到低维 n'。


PCA(Principal Component Analysis)，即主成分分析方法，是一种使用最广泛的数据降维算法，也可以用于提取数据的主要特征。PCA 的主要思想是将n维特征映射到k维上，这k维是全新的正交特征也被称为主成分，是在原有n维特征的基础上重新构造出来的k维特征。PCA的工作就是从原始的空间中顺序地找一组相互正交的坐标轴，新的坐标轴的选择与数据本身是密切相关的。其中，第一个新坐标轴选择是原始数据中方差最大的方向，第二个新坐标轴选取是与第一个坐标轴正交的平面中使得方差最大的，第三个轴是与第1,2个轴正交的平面中方差最大的。依次类推，可以得到n个这样的坐标轴。通过这种方式获得的新的坐标轴，我们发现，大部分方差都包含在前面k个坐标轴中，后面的坐标轴所含的方差几乎为0。于是，我们可以忽略余下的坐标轴，只保留前面k个含有绝大部分方差的坐标轴。事实上，这相当于只保留包含绝大部分方差的维度特征，而忽略包含方差几乎为0的特征维度，实现对数据特征的降维处理。



缓解维度灾难：PCA 算法通过舍去一部分信息之后能使得样本的采样密度增大（因为维数降低了），这是缓解维度灾难的重要手段；
降噪：当数据受到噪声影响时，最小特征值对应的特征向量往往与噪声有关，将它们舍弃能在一定程度上起到降噪的效果；
过拟合：PCA 保留了主要信息，但这个主要信息只是针对训练集的，而且这个主要信息未必是重要信息。有可能舍弃了一些看似无用的信息，但是这些看似无用的信息恰好是重要信息，只是在训练集上没有很大的表现，所以 PCA 也可能加剧了过拟合；
特征独立：PCA 不仅将数据压缩到低维，它也使得降维之后的数据各特征相互独立；


优缺点：
优点：  
- 缓解维度灾难带来的问题(涉及计算距离的算法、运算时间)
- 各主成分之间正交，互不相干，可解决原有数据的特征相关问题
>

缺点：
- 主成分的含义具有模糊性，建模后难以解释。如果后续要解释模型中每个特征的含义，考虑换种方法吧。  
- 可能舍弃掉重要信息。方差小的非主成分在训练集作用不大，但可能其在测试集、实际数据中起重要作用，在训练集觉得他们的去留，可能最后模型过拟合。  
- 
PCA算法的主要优点有：

　　　　1）仅仅需要以方差衡量信息量，不受数据集以外的因素影响。　

　　　　2）各主成分之间正交，可消除原始数据成分间的相互影响的因素。

　　　　3）计算方法简单，主要运算是特征值分解，易于实现。

　　　　PCA算法的主要缺点有：

　　　　1）主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强。

　　　　2）方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响。
