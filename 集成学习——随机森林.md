## 随机森林 Random Forest
![](https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fimage1.bubuko.com%2Finfo%2F202006%2F20200618220515453491.jpg&refer=http%3A%2F%2Fimage1.bubuko.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1618987049&t=2aae50e8f30d2a067cffbd6b43cbfcd0)

### 目录
- 概述
- 算法优缺点
- sklearn用法

----------------
### 一. 概述
随机森林是 Bagging 的一个拓展变体，也可以说是进阶版。它在原来 Bagging 的基础上，做出一些改变，得到更高的性能。  
改变：  
(1)选定 CART 决策树作为基学习器    
(2)在决策树训练过程中,引入随机属性选择。在传统决策树里，划分点的选择都是在当前节点的属性集合里选择一个最优的属性(假设总共有 d 个属性);而随机森林，先从该节点的属性集合里随机选择 k 个属性作为属性子集，再从这个子集里选择一个最优属性来作分支属性。(如果 k=d,则基决策树的构建与传统决策树相同。一般，当 d 越小，方差越小，但是偏倚会相对大一点)         

说明：  
- 随机森林的基学习器的多样性，一方面来自样本的扰动，一方面来自属性扰动。使得最终集成的泛化性能可通过个体学习器之间差异度的增加而进一步提升。(随机森林的随机性可以这样理解的，一个随机选择样本，一个随机选择特征)  
- 随着基学习器数目的增加，随机森林通常会收敛到更低的泛化误差。在基学习器较少时，因为引入了属性扰动，个体学习器的性能往往有所降低，但是随着基学习器数目增多，性能会好起来。  

### 二. 算法优缺点：  
#### 2.1 优点：  
- (1) 随机森林可高度并行，处理大数据集时很有优势
- (2) 训练效率高，因为属性的随机选择，在特征特别多时优势明显可见、
- (3) 对部分特征缺失不敏感
- (4) 训练出的模型方差小，泛化能力强
- (5) 可输出特征的重要性，在实际应用中方便决策

#### 2.2 缺点：  
- (1) 噪声较大时，容易过拟合
- (2) 取值划分比较多的特征易对RF的决策产生大的影响，影响建模效果  


参考资料：  
西瓜书    
[刘建平blog](https://www.cnblogs.com/pinard/p/6156009.html)   
