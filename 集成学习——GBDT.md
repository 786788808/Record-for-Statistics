## 梯度提升树 GBDT(Gradient Boosting Decison Tree)
梯度提升树(Gradient Boosting Decison Tree)是提升树(Boosting Tree)的一种改进算法，先讲一讲提升树。  
提升树就是通过不断建立树来不断拟合前一个问题的残差来不断接近目标。
先来个通俗理解：假如有个人30岁，我们第一次用20岁去拟合，发现损失有10岁。于是，我们用6岁去拟合剩下的损失，发现差距还有4岁。第三次，我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。最后将每次拟合的岁数加起来便是模型输出的结果。  

提升方法采用加法模型(基函数的线性组合)与前向分步算法,以决策树为基函数的提升方法即提升树。在分类问题中，其决策树是二叉分类树；在回归问题中，其决策树是二叉回归树。  
不同问题的提升树学习方法使用的损失函数不同，回归问题一般用平方误差损失函数，分类问题一般用指数损失函数，以及其它一般决策问题的一般损失函数。    
当损失函数是平方损失函数或者指数损失函数时，梯度提升树每一步优化是很简单的。但是，当损失函数是一般损失函数时，往往每一步优化起来不那么容易。针对这一问题，大神 Friedman 提出了梯度提升树算法，利用梯度下降法来解决问题，其关键点是利用损失函数的负梯度作为提升树算法中的残差的近似值，不断去减小残差。(与Adaboost，提高被分类错误样本的权重与降低被分类正确样本的权重，有很大不同，remember)  

无论是解决分类还是回归问题，GBDT 都是采用CART回归树。
> 为什么不是分类问题用分类树，回归问题用回归树呢？
> 因为 GBDT 每次迭代要拟合的是梯度值，这是连续值，所以要用回归树解决。

