## 梯度提升树 GBDT(Gradient Boosting Decison Tree)

### 一. 什么是 GBDT：
梯度提升树(Gradient Boosting Decison Tree)是提升树(Boosting Tree)的一种改进算法，先讲一讲提升树。  
提升树就是通过不断建立树来不断拟合前一个问题的残差来不断接近目标。
先来个通俗理解：假如有个人30岁，我们第一次用20岁去拟合，发现损失有10岁。于是，我们用6岁去拟合剩下的损失，发现差距还有4岁。第三次，我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。最后将每次拟合的岁数加起来便是模型输出的结果。  

提升方法采用加法模型(基函数的线性组合)与前向分步算法,以决策树为基函数的提升方法即提升树。在分类问题中，其决策树是二叉分类树；在回归问题中，其决策树是二叉回归树。  
不同问题的提升树学习方法使用的损失函数不同，回归问题一般用平方误差损失函数，分类问题一般用指数损失函数，以及其它一般决策问题的**一般损失函数**。    
当损失函数是平方损失函数或者指数损失函数时，梯度提升树每一步优化是很简单的。但是，当损失函数是一般损失函数时，往往每一步优化起来不那么容易。针对这一问题，大神 Friedman 提出了梯度提升树算法，利用梯度下降法来解决问题，其关键点是**利用损失函数的负梯度作为提升树算法中的残差的近似值，不断去减小残差**。(与Adaboost，提高被分类错误样本的权重与降低被分类正确样本的权重，有很大不同，remember)  

所以，GBDT，是为了解决一般损失函数的优化问题而出现的一种算法。其用损失函数的负梯度在当前模型的值来模拟回归问题中残差的近似值。(梯度问题，理论的去看B站、书讲解。形象点理解，可以用下山的例子来理解，假设你要从山顶下山，你想找最快的一条路，寻思往哪个方向一步一步地走下去。这时候就用梯度的负方向来解决。梯度的负方向会指向最陡峭最易下山的方向，走一步后再重新调整方向，一步一步朝这个方向走。

无论是解决分类还是回归问题，**GBDT 都是采用CART回归树**。
> 为什么不是分类问题用分类树，回归问题用回归树呢？
> 因为 GBDT 每次迭代要拟合的是梯度值，这是连续值，所以要用回归树解决。  

### 二. sklearn 用法：
sklearn 用法：  
在 sklearn 中，GradientBoostingClassifier 用于分类问题， 而 GradientBoostingRegressor 用于回归问题。  
它们的参数由boosting框架参数和弱学习器参数组成，而弱学习器是CART回归决策树，所以调参就是调框架参数和CART决策树参数。   
>
#### 2.1 框架参数
- n_estimators: 指定弱学习器的个数。值太小，易欠拟合；值太大，易过拟合。默认值 100。一般要和 learning_rate一起调参。
- learning_rate: 指定每个弱学习器的权重缩减系数ν，也称作步长。强学习器的迭代公式为 fk(x) = fk−1(x) + νhk(x)。ν 的取值范围：(0,1]。对于同样的训练集拟合效果，步长 ν 小，弱学习器 n_estimators 个数就要大；步长 ν 大，弱学习器 n_estimators 个数就要大。
- subsample: 指定子采样值，范围：(0,1]。有别于随机森林，这里用的是不放回抽样，推荐设置 0.5-0.8。取值小于1，是因为可以减小方差，防止过拟合，增强模型的泛化能力，但是抽样过少，又会造成偏差过大。所以推荐0.5-0.8这个范围。  
- loss: 指定 GBDT 算法损失函数。分类问题和回归问题用的损失函数不一样，所以要区别。    
  - 分类模型：有对数似然损失函数 deviance 和指数损失函数 exponential 两者输入选择。一般用对数似然函数，用指数损失函数 exponential 的话，就回到 Adaboost 了。
  - 回归模型：有均方差 ls、绝对损失 lad、Huber 损失 huber 和分位数损失 quantile 。一般用均方差即可。如果噪点较多，推荐 huber 损失函数。如果需要对训练集进行分段预测，则用 quantile。
- alpha：这个参数只有 GradientBoostingRegressor 有，当使用 Huber 损失和分位数损失时，需要指定分位数的值。默认是 0.9，如果噪音点较多，可以适当降低这个分位数的值。

#### 2.2 弱学习器参数  
这里的都是CART回归树的参数，基本调下列参数即可：  
- max_features 划分时考虑的最大特征数: 有 None、log2、sqrt、auto。如果不是高维情况，如维度小于 50，用默认的 None 即可；如果特征数比较多，就用其他参数来选择部分特征训练模型，减少训练时间。  
- max_depth 决策树最大深度: 默认值 3。一般来说，数据少 or 特征少的时候不管这个参数。如果模型样本量多，特征也多的情况下，可以设置到 10-100 之间。
- min_samples_split 内部节点再划分所需最小样本数: 如果某节点的样本数少于 min_samples_split，树停止划分。 默认值是 2，如果样本量不大，基本不修改；如果样本量非常大，建议增大该值。  
- min_samples_leaf 叶子节点最少样本数: 设定叶子节点需要的最小样本数，如果某叶子节点里包含的样本数小于指定值，该叶子节点和其兄弟节点会被剪掉。，如果样本量不大，基本不修改；如果样本量非常大，建议增大该值。 
- min_weight_fraction_leaf 叶子节点最小的样本权重和：这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。 默认是 0，就是不考虑权重问题。如果样本缺失值较多，或者分类样本类别很不平衡，就会引入样本权重，要调这个参数。    
- max_leaf_nodes 最大叶子节点数: 默认是 None，即不限制最大的叶子节点数，可防止过拟合。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到。
- min_impurity_split 节点划分最小不纯度: 可限制决策树的增长，如果某节点的不纯度(基于基尼系数，均方差)小于这个阈值，则该节点不再生成子节点。即为叶子节点 。一般用默认值1e-7就行。




