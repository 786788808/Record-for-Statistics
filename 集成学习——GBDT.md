## 梯度提升树 GBDT(Gradient Boosting Decison Tree)

### 一. 什么是 GBDT：
梯度提升树(Gradient Boosting Decison Tree)是提升树(Boosting Tree)的一种改进算法，先讲一讲提升树。  
提升树就是通过不断建立树来不断拟合前一个问题的残差来不断接近目标。
先来个通俗理解：假如有个人30岁，我们第一次用20岁去拟合，发现损失有10岁。于是，我们用6岁去拟合剩下的损失，发现差距还有4岁。第三次，我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。最后将每次拟合的岁数加起来便是模型输出的结果。  

提升方法采用加法模型(基函数的线性组合)与前向分步算法,以决策树为基函数的提升方法即提升树。在分类问题中，其决策树是二叉分类树；在回归问题中，其决策树是二叉回归树。  
不同问题的提升树学习方法使用的损失函数不同，回归问题一般用平方误差损失函数，分类问题一般用指数损失函数，以及其它一般决策问题的**一般损失函数**。    
当损失函数是平方损失函数或者指数损失函数时，梯度提升树每一步优化是很简单的。但是，当损失函数是一般损失函数时，往往每一步优化起来不那么容易。针对这一问题，大神 Friedman 提出了梯度提升树算法，利用梯度下降法来解决问题，其关键点是**利用损失函数的负梯度作为提升树算法中的残差的近似值，不断去减小残差**。(与Adaboost，提高被分类错误样本的权重与降低被分类正确样本的权重，有很大不同，remember)  

所以，GBDT，是为了解决一般损失函数的优化问题而出现的一种算法。其用损失函数的负梯度在当前模型的值来模拟回归问题中残差的近似值。(梯度问题，理论的去看B站、书讲解。形象点理解，可以用下山的例子来理解，假设你要从山顶下山，你想找最快的一条路，寻思往哪个方向一步一步地走下去。这时候就用梯度的负方向来解决。梯度的负方向会指向最陡峭最易下山的方向，走一步后再重新调整方向，一步一步朝这个方向走。

无论是解决分类还是回归问题，**GBDT 都是采用CART回归树**。
> 为什么不是分类问题用分类树，回归问题用回归树呢？
> 因为 GBDT 每次迭代要拟合的是梯度值，这是连续值，所以要用回归树解决。  

### 二. sklearn 用法：
sklearn 用法：  
在 sklearn 中，GradientBoostingClassifier 用于分类问题， 而 GradientBoostingRegressor 用于回归问题。  
它们的参数由boosting框架参数和弱学习器参数组成，而弱学习器是CART回归决策树，所以调参就是调框架参数和CART决策树参数。   
>
#### 2.1 框架参数
- n_estimators: 指定弱学习器的个数。值太小，易欠拟合；值太大，易过拟合。默认值 100。一般要和 learning_rate一起调参。
- learning_rate: 指定每个弱学习器的权重缩减系数ν，也称作步长。强学习器的迭代公式为 fk(x) = fk−1(x) + νhk(x)。ν 的取值范围：(0,1]。对于同样的训练集拟合效果，步长 ν 小，弱学习器 n_estimators 个数就要大；步长 ν 大，弱学习器 n_estimators 个数就要大。
- subsample: 指定子采样值，范围：(0,1]。有别于随机森林，这里用的是不放回抽样，推荐设置 0.5-0.8。取值小于1，是因为可以减小方差，防止过拟合，增强模型的泛化能力，但是抽样过少，又会造成偏差过大。所以推荐0.5-0.8这个范围。  
- loss: 指定 GBDT 算法损失函数。分类问题和回归问题用的损失函数不一样，所以要区别。    
  - 分类模型：有对数似然损失函数 deviance 和指数损失函数 exponential 两者输入选择。一般用对数似然函数，用指数损失函数 exponential 的话，就回到 Adaboost 了。
  - 回归模型：有均方差 ls、绝对损失 lad、Huber 损失 huber 和分位数损失 quantile 。一般用均方差即可。如果噪点较多，推荐 huber 损失函数。如果需要对训练集进行分段预测，则用 quantile。
- alpha：这个参数只有 GradientBoostingRegressor 有，当使用 Huber 损失和分位数损失时，需要指定分位数的值。默认是0.9，如果噪音点较多，可以适当降低这个分位数的值。

#### 2.2 弱学习器参数  



