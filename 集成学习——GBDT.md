## 梯度提升树 GBDT(Gradient Boosting Decison Tree)
梯度提升树(Gradient Boosting Decison Tree)是提升树(Boosting Tree)的一种改进算法，先讲一讲提升树。  
提升树就是通过不断建立树来不断拟合前一个问题的残差来不断接近目标。
先来个通俗理解：假如有个人30岁，我们第一次用20岁去拟合，发现损失有10岁。于是，我们用6岁去拟合剩下的损失，发现差距还有4岁。第三次，我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。最后将每次拟合的岁数加起来便是模型输出的结果。  

提升方法采用加法模型(基函数的线性组合)与前向分步算法,以决策树为基函数的提升方法即提升树。在分类问题中，其决策树是二叉分类树；在回归问题中，其决策树是二叉回归树。  
不同问题的提升树学习方法使用的损失函数不同，回归问题一般用平方误差损失函数，分类问题一般用指数损失函数，以及其它一般决策问题的**一般损失函数**。    
当损失函数是平方损失函数或者指数损失函数时，梯度提升树每一步优化是很简单的。但是，当损失函数是一般损失函数时，往往每一步优化起来不那么容易。针对这一问题，大神 Friedman 提出了梯度提升树算法，利用梯度下降法来解决问题，其关键点是**利用损失函数的负梯度作为提升树算法中的残差的近似值，不断去减小残差**。(与Adaboost，提高被分类错误样本的权重与降低被分类正确样本的权重，有很大不同，remember)  

所以，GBDT，是为了解决一般损失函数的优化问题而出现的一种算法。其用损失函数的负梯度在当前模型的值来模拟回归问题中残差的近似值。(梯度问题，理论的去看B站、书讲解。形象点理解，可以用下山的例子来理解，假设你要从山顶下山，你想找最快的一条路，寻思往哪个方向一步一步地走下去。这时候就用梯度的负方向来解决。梯度的负方向会指向最陡峭最易下山的方向，走一步后再重新调整方向，一步一步朝这个方向走。

无论是解决分类还是回归问题，**GBDT 都是采用CART回归树**。
> 为什么不是分类问题用分类树，回归问题用回归树呢？
> 因为 GBDT 每次迭代要拟合的是梯度值，这是连续值，所以要用回归树解决。  


sklearn 用法：  
在 sklearn 中，GradientBoostingClassifier 用于分类问题， 而 GradientBoostingRegressor 用于回归问题。  
它们的参数由boosting框架参数和弱学习器参数组成，而弱学习器是CART回归决策树，所以调参就是调框架参数和CART决策树参数。   



