## K-Means聚类————基于距离的算法（十大经典算法之一）
K-Means 聚类采用距离作为相似性的评价指标，即认为两个对象的距离越近，其相似度就越大。
### 一. 原理步骤：  
这是一个循环迭代的算法，简单易懂。假定我们要对 N 个样本观测做聚类，要求聚为 K 类：  
(1) 选择 K 个点作为初始中心点（最原始的是随机选择）；  
(2) 接下来，按照距离初始中心点最小的原则，把所有观测分到各中心点所在的类中；  
(3) 每类中有若干个观测，计算 K 个类中所有样本点的均值，作为第二次迭代的 K 个中心点；  
(4) 然后根据这个中心重复第 2、3 步，直到收敛（中心点不再改变或达到指定的迭代次数），聚类过程结束。  
下图展示最简单的聚类，分 2 类：  
![](https://ftp.bmp.ovh/imgs/2020/12/12cb6a37de432746.png)
实际情况中，质心一般要不断迭代，才能达到相对最优情况。
>
### 二. 算法优缺点：
(2.1) 优点：  
- 该算法时间复杂度为O(tkmn)，（其中，t为迭代次数，k为簇的数目，m为记录数，n为维数）与样本数量线性相关，所以，对于处理大数据集合，该算法非常高效    
- 原理简单，较易理解    
>
(2.2) 缺点：  
- 易受异常值和噪声影响 （异常值使均值偏离）   
- 结果不一定是全局最优，只能保证局部最优（和初始点选取有关）
- 聚类中心的个数 K 需要事先给定，但在实际应用中这个 K 值的选定是非常难以估计的
- 不适于发现非凸面形状的簇或大小差别很大的簇（不均衡样本）
- k 个初始化的质心的位置选择对最后的聚类结果和运行时间都有很大的影响,质心如果太近会影响聚类效果。
>
(2.3) 改善方法：  
- 初始选择质心问题：针对最初随机选取 k 个质心可能使收敛变慢、模型效果不佳问题，因此推出 K-Means++ 算法，其优化了初始质心的选择，更合理去选择，从而优化模型。scikit-learn 默认使用 K-Means++ 算法，也可调回 random ，变回最原始的算法。
- 计算速度优化——1：传统的K-Means需要不断计算所有点到质心的距离，新的elkan K-Means利用三角形性质：两边之和大于等于第三边,以及两边之差小于第三边，来减少距离的计算，有效提高迭代速度。针对稠密数据可用，若是稀疏数据，有缺失值，该方法行不通，只能用回原来的距离计算方法。详见大神博客：https://www.cnblogs.com/pinard/p/6164214.html
- 计算速度优化——2：样本量很大的话，也需要消耗较长计算时间，比如样本量达到10万、特征有100以上，可以考虑用Mini Batch K-Means。它以抽样的方式选出样本再计算，可以减少计算量，提高迭代速度。这会牺牲掉一定的精度，为了增加算法的准确性，一般会多跑几次，用得到不同的随机采样集来得到聚类簇，选择其中最优的聚类簇。
>
### 二. k值评估：







