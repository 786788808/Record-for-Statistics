K近邻法(KNN)是一种很基本的机器学习算法，属于监督学习类算法，是一种简单易懂的方法，可用于回归和分类。比如我们要给点 A 做预测，做**分类**的时候，我们经常采用**少数服从多数**原则，A点最近的 K 个点属于哪个类最多，A点就属于那个类；做**回归**的时候，一般采用附近K个点的**平均值**作为A点的回归值。    
很多人会把 KNN 与 KMeans 混合，记住 KNN：K 是指附近的 K 个点，有分类标签；KMeans：K 是指分成 K 类，无分类标签。  

### 一. KNN 原理
#### (1.1) 基本步骤：
简单来说，就是当预测一个数据点 A 的时候，根据它距离最近的 K 个点是什么类别来判断 x 属于哪个类别。  
举个例子：  
我们手上有两种品种的芒果，从两个维度x y去衡量辨别品种，现有一个新的芒果A，想根据KNN去判定它属于哪一类别。三角形表示甲品种，爱心表示乙品种。  
过程：
- 取K值为3时，计算每个样本点到点A的距离，取最近的3个点作为评判点，现在有2个点是三角形，1个点是爱心，则判定点A属于甲品种。  
- 取K值为6时，计算每个样本点到点A的距离，取最近的6个点作为评判点，现在有4个点是爱心形，2个点是三角形，则判定点A属乙品种。
![](https://ftp.bmp.ovh/imgs/2021/01/0b5b9aa83df87f33.png)  

#### (1.2) 算法关键点：  
- (1.2.1) K值的选择
K值越小，越容易过拟合。当K值越小，模型越复杂，也容易过拟合，不利于实际应用。可通过交叉验证选择合适的K值。    
- (1.2.2) 距离的选择
距离常见的有：曼哈顿距离、欧式距离、闵可夫斯基距离(一个通式，取2时，就是欧式距离)、切比雪夫距离、标准化欧式距离。  
一般直接选欧式距离即可。  
- (1.2.3) 决策原则
在分类算法里，一般采用少数服从多数原则

#### (1.3) **注意：**
在计算距离的算法里，必须要做归一化or标准化。如果漏掉这一步，算法会偏向于取值范围较大的特征，影响模型效果。   

### 二. KNN实现方法
#### (2.1) 蛮力实现  
我们上述的实现过程就是采用蛮力实现，逐个计算样本点到预测点的距离，取最近的K个点来决策。  
当样本量少、特征少的时候，用蛮力实现还是可以的。  
但是样本量达到几十万、特征几千个的时候，计算机可得算惨了。  
于是有新的方法产生。  

#### (2.2) KD 树

#### (2.3) 球树
KNN 算法中，我们不需要对数据分布做任何假设。其次，这是一个惰性算法，拿逻辑回归来说，逻辑回归需要训练再到预测，而 KNN 算法拿到数据后，基本没有训练一说，做预测的时候才开始算。  
优缺点：  
KNN算法是很基本的机器学习算法了，它非常容易学习，在维度很高的时候也有很好的分类效率，因此运用也很广泛，这里总结下KNN的优缺点。

 
### 三. 优缺点：  
#### (3.1) 优点有：
1） 理论成熟，好理解，如果客户想要了解分类过程，用这个算法解释还是较好讲解的
2） 可用于线性、非线性分类
3） 训练时间复杂度比支持向量机之类的算法低，仅为O(n)
4） 和朴素贝叶斯之类的算法比，对数据没有假设，准确度高，对异常点不敏感
5） 由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，KNN方法较其他方法更为适合
6）该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量较小的类域采用这种算法比较容易产生误分
　　　　

　　　　KNN的主要缺点有：

　　　　1）计算量大，尤其是特征数非常多的时候

　　　　2）样本不平衡的时候，对稀有类别的预测准确率低

　　　　3）KD树，球树之类的模型建立需要大量的内存

　　　　4）使用懒散学习方法，基本上不学习，导致预测时速度比起逻辑回归之类的算法慢

　　　　5）相比决策树模型，KNN模型可解释性不强
　　　　以上就是KNN算法原理的一个总结，希望可以帮到朋友们，尤其是在用scikit-learn学习KNN的朋友们。
