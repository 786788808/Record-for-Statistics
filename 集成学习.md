## 集成学习 Ensemble Learning

### 目录
- 一. Bagging
- 二. Boosting
- 三. 算法结合策略

 
集成学习不是单独的一个机器学习算法，其按照不同的思路来组合多个弱学习器来完成学习任务。     
现有三种常见的集成框架：：Bagging，Boosting 和 Stacking，暂时先写前两种，后面再补其他。    
![](https://ae03.alicdn.com/kf/Ued37aabb90b14dbfbf18924fb3b104249.jpg)    

----------------

### 一. Bagging
先补充一个抽样概念：
- Bootstrap 自助法
> 一种有放回的抽样方法，它是非参数统计中一种重要的估计统计量方差进而进行区间估计的统计方法。在小样本时效果很好。    
> 过程：  
> 给定包含 m 个样本的初始数据集 D，现要对它进行采样产生数据集 D'：每次随机地从 D 中抽取一个样本，放进新数据集 D'，然后再将该样本放回到初始样本 D 中，使得该样本后面仍有可能被抽取。重复以上抽取动作，直至新数据集 D'中包含 m 个样本。  
> 在这个过程中，部分样本会被重复抽取，而部分不出现。可以做估计：样本在 m 次采样中始终不被采用的概率是 (1-1/m)^m。取极限可约等于 0.368，即可认为数据集 D 中 36.8% 的样本始终不会被抽到,有 63.2% 的样本会出现在新数据集中。  
-------------
Bagging 工作机制：  
采样出 T 个含 m 个训练样本的采样集，然后基于每个采样集训练出一个基学习器，再将这些基学习器经一定的结合策略形成一个强学习器。    
![](https://sc04.alicdn.com/kf/Ud65d6ec327764013bb3bb720412424e8Z.jpg)  
说明：  
- (1) Bagging 的个体学习器之间不存在强依赖关系，可并行生成。(不能说相互独立，因为子训练集之间是有交集的)
- (2) 弱学习器的准确率须不低于50%。弱学习器常指泛化性能略优于随机猜测的学习器，例如在二分类问题上精度略高于 50% 的分类器。太差的弱学习器掺杂在一起，最终出来的模型效果并不会太好。     
- (3) 上面的随机抽样一般是指 Boostrap 放回抽样，留意其未被抽样的样本，调参中可用这个参数优化模型。    
- (4) 使用模型的偏差和方差来描述其在训练集上的准确度和防止过拟合的能力。对于 Bagging 来说，整体模型的偏差和基模型近似，随着训练的进行，整体模型的方差降低，稳定性较好。当单个评估器存在过拟合问题时，能够一定程度上解决过拟合问题。   
- (5) 随机森林就是在 Bagging 基础上构建出来的方法，其性能更佳。  
>

### 二. Boosting
不同于 Bagging 的相对独立，Boosting 的个体学习器之间存在较强的依赖性、必须串行生成。(这也是 Bagging 和 Boosting 最大的不同点)   
Boosting 工作机制：  
先从初始训练集用初始权重训练出一个基学习器1，再根据基学习器的表现(学习误差率)来更新训练样本的权重，使得先前基学习器做错的训练样本在后面的权重变高，让他们得到更多的重视。然后在调整后的样本进行新一轮训练得到新的基学习器。后面不断重复上面的操作，直至基学习器数目达到指定的数目T。最终将这 T 个弱学习器通过集合策略进行整合，得到最终的强学习器。      
![](https://ae03.alicdn.com/kf/U29b89572558c473eb237adacc6ec40f1y.jpg)  
说明：   
- (1) 代表算法：AdaBoost 和提升树(boosting tree)系列算法  
- (2) 对于 Boosting 来说，整体模型的初始偏差较高，方差较低，随着训练的进行，整体模型的偏差降低（虽然也不幸地伴随着方差增高）。当训练过度时，因方差增高，整体模型的准确度反而降低

### 三. 算法结合策略
#### 3.1 平均法
在处理回归问题时，一般使用平均法，即对于若干个弱学习器的输出进行平均得到最终的预测输出。  
假设最终得到n个弱分类器{h1, h2, …, hT},
- 算数平均，即
- 加权平均

可直接做算数平均，如果基学习器有权重，也可以加权重算。    

#### 3.2 投票法
在处理回归问题时，一般使用投票法。
- 相对多数投票法，即少数服从多数，若不止一个获得最高票，则随机选择一个做最终类别。
- 绝对多数投票法，在相对多数投票的基础上，要满足过半数票，不然无效。
- 加权投票法，类似加权平均法，每个弱学习器的分类票数要乘以一个权重，最终将各个类别的加权票数求和，最大的值对应的类别为最终类别。

#### 3.3 学习法
前两种方法都是对弱学习器的结果做平均或者投票，相对比较简单，但是可能学习误差比较大，于是推出了学习法，代表方法是stacking。     
stacking将训练集弱学习器的学习结果作为输入，将训练集的输出作为输出，重新训练一个学习器来得到最终结果。    
我们将弱学习器称为初级学习器，将用于结合的学习器称为次级学习器。在实际应用中，对于测试集，我们先用初级学习器预测一次，得到次级学习器的输入样本，再用次级学习器预测一次，得到最终的预测结果。  


参考资料：  
https://www.zhihu.com/question/29036379/answer/111637662  
[刘建平blog](https://www.cnblogs.com/pinard/p/6131423.html)
