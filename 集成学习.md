## 集成学习 Ensemble Learning
集成学习不是单独的一个机器学习算法，其按照不同的思路来组合多个弱学习器来完成学习任务。  
![](https://ae03.alicdn.com/kf/U401735f481d1477ca0b0a31448a36d03O.jpg)  
现有三种常见的集成框架：：Bagging，Boosting 和 Stacking.  

### 一. Bagging
先补充一个抽样概念：
- Bootstrap自助法
> 一种有放回的抽样方法，它是非参数统计中一种重要的估计统计量方差进而进行区间估计的统计方法。在小样本时效果很好。    
> 过程：  
> 给定包含m个样本的初始数据集D，现要对它进行采样产生数据集D'：每次随机地从D中抽取一个样本，放进新数据集D'，然后再将该样本放回到初始样本D中，使得该样本后面仍有可能被抽取。重复以上抽取动作，直至新数据集D'中包含m个样本。  
> 在这个过程中，部分样本会被重复抽取，而部分不出现。可以做估计：样本在m次采样中始终不被采用的概率是(1-1/m)^m。取极限可约等于0.368，即可认为数据集D中36.8%的样本始终不会被抽到,有63.2%的样本会出现在新数据集中。  
-------------
Bagging 工作机制：  
采样出T个含m个训练样本的采样集，然后基于每个采样集训练出一个基学习器，再将这些基学习器经一定的结合策略形成一个强学习器。    
![](https://ae02.alicdn.com/kf/U929d989ac20647ada5d47c9234bf94e7T.jpg)  
说明：  
- (1) Bagging的个体学习器之间不存在强依赖关系，可并行生成。(不能说相互独立，因为子训练集之间是有交集的)
- (2) 弱学习器常指泛化性能略优于随机猜测的学习器，例如在二分类问题上精度略高于50%的分类器。太差的弱学习器掺杂在一起，最终出来的模型效果并不会太好。     
- (3) 上面的随机抽样一般是指Boostrap放回抽样，留意其未被抽样的样本，调参中可用这个参数优化模型。    
- (4) 在方差和偏差这一块，Bagging保证了较低的方差，比较稳。   
- (5) 随机森林就是在Bagging基础上构建出来的方法，其性能更加，后面提及。  
>

### 二. Boosting
不同于Bagging的相对独立，Boosting的个体学习器之间存在较强的依赖性、必须串行生成。(这也是Bagging和Boosting最大的不同点)   
Boosting 工作机制：  
先从初始训练集用初始权重训练出一个基学习器1，再根据基学习器的表现(学习误差率)来更新训练样本的权重，使得先前基学习器做错的训练样本在后面的权重变高，让他们得到更多的重视。然后在调整后的样本进行新一轮训练得到新的基学习器。后面不断重复上面的操作，直至基学习器数目达到指定的数目T。最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。      

